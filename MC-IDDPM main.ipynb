{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c3773b",
   "metadata": {},
   "source": [
    "# Here are the library you need to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8361ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import scipy.io\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "from timm.models.layers import DropPath\n",
    "from einops import rearrange\n",
    "from scipy import ndimage\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from natsort import natsorted\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from timm.models.layers import DropPath, to_3tuple, trunc_normal_\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    "    RandAffined,\n",
    "    RandCropByLabelClassesd,\n",
    "    SpatialPadd,\n",
    "    RandAdjustContrastd,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    Resized,\n",
    "    Transposed,\n",
    "    RandSpatialCropd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    ResizeWithPadOrCropd\n",
    ")\n",
    "from monai.transforms import (CastToTyped,\n",
    "                              Compose, CropForegroundd, EnsureChannelFirstd, LoadImaged,\n",
    "                              NormalizeIntensity, RandCropByPosNegLabeld,\n",
    "                              RandFlipd, RandGaussianNoised,\n",
    "                              RandGaussianSmoothd, RandScaleIntensityd,\n",
    "                              RandZoomd, SpatialCrop, SpatialPadd, EnsureTyped)\n",
    "from monai.transforms.compose import MapTransform\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from skimage.transform import resize\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "from diffusion.Create_diffusion import *\n",
    "from diffusion.resampler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e4aa7",
   "metadata": {},
   "source": [
    "# Build the data loader using the monai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a750f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the dataloader hyper-parameters, including the batch size,\n",
    "# image size, image spacing (don't forget to adjust the spacing to your desired number)\n",
    "BATCH_SIZE_TRAIN = 4*1\n",
    "img_size = (64,64,4)\n",
    "spacing = (2,2,2)\n",
    "patch_num = 2\n",
    "channels = 1\n",
    "metric = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720eabef",
   "metadata": {},
   "source": [
    "# Data processing for mat files (contain {'image', 'label'}) (for nii files, in the next block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1b3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use pre-processed matlab file, which has already normalized to -1 to 1, with same spacing, same orientations.\n",
    "# The reason we do that is because it can save us time to processing the data on the fly. If you don't like it,\n",
    "# we provide the standard processing pipeline for nii.gz files below\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,imgs_path,labels_path, train_flag = True):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.labels_path = labels_path\n",
    "        self.train_flag = train_flag\n",
    "        file_list = natsorted(glob.glob(self.imgs_path + \"*\"), key=lambda y: y.lower())     \n",
    "        label_list = natsorted(glob.glob(self.labels_path + \"*\"), key=lambda y: y.lower())\n",
    "\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for img_path in file_list:\n",
    "            class_name = img_path.split(\"/\")[-1]\n",
    "            self.data.append([img_path, class_name])\n",
    "        for label_path in label_list:\n",
    "                class_name = label_path.split(\"/\")[-1]\n",
    "                self.label.append([label_path, class_name])\n",
    "        self.train_transforms = Compose(\n",
    "                [\n",
    "                    AddChanneld(keys=[\"image\",\"label\"]),\n",
    "                    ResizeWithPadOrCropd(\n",
    "                          keys=[\"image\",\"label\"],\n",
    "                          spatial_size=(192,192,4),\n",
    "                          constant_values = -1,\n",
    "                    ),\n",
    "                    RandSpatialCropSamplesd(keys=[\"image\",\"label\"],\n",
    "                                      roi_size = img_size,\n",
    "                                      num_samples = patch_num,\n",
    "                                      random_size=False,\n",
    "                                      ),\n",
    "                    ToTensord(keys=[\"image\",\"label\"]),\n",
    "                ]\n",
    "            )\n",
    "        self.test_transforms = Compose(\n",
    "                [\n",
    "                    AddChanneld(keys=[\"image\",\"label\"]),\n",
    "                    ResizeWithPadOrCropd(\n",
    "                          keys=[\"image\",\"label\"],\n",
    "                          spatial_size=(192,192,168),\n",
    "                          constant_values = -1,\n",
    "                    ),\n",
    "                    ToTensord(keys=[\"image\",\"label\"]),\n",
    "                ]\n",
    "            ) \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, class_name = self.data[idx]\n",
    "\n",
    "        cao = scipy.io.loadmat(img_path)\n",
    "\n",
    "        if not self.train_flag:\n",
    "            affined_data_dict = self.test_transforms(cao)   \n",
    "            img_tensor = affined_data_dict['image'].to(torch.float)\n",
    "            label_tensor = affined_data_dict['label'].to(torch.float)\n",
    "        else:\n",
    "            affined_data_dict = self.train_transforms(cao)   \n",
    "            img = np.zeros([patch_num, img_size[0], img_size[1], img_size[2]])\n",
    "            label = np.zeros([patch_num, img_size[0], img_size[1], img_size[2]])\n",
    "            for i,after_l in enumerate(affined_data_dict):\n",
    "                img[i,:,:,:] = after_l['image']\n",
    "                label[i,:,:,:] = after_l['label']\n",
    "            img_tensor = torch.unsqueeze(torch.from_numpy(img.copy()), 1).to(torch.float)\n",
    "            label_tensor = torch.unsqueeze(torch.from_numpy(label.copy()), 1).to(torch.float)\n",
    "\n",
    "        \n",
    "        return img_tensor,label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57621805",
   "metadata": {},
   "source": [
    "# Data processing for nii files (including reading, adding channels, align orientation, align spacing, normalization (MRI and CT  has different normalization), cropping and padding, and finally extracing patches for training.\n",
    "\n",
    "# But trust me, this process takes a lot of time. Try to process all the data before you run the model instead of processing them on-the-fly. At least try to get rid of the spacing and orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e94c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only be careful about the ResizeWithPadOrCropd. I am not sure should you use it or not. In my case,\n",
    "# # I need a volume with fixed size.\n",
    "\n",
    "# # One more thing, be careful of the normalization, CT is quantative and MRI is not, so they need different normalization here.\n",
    "# # Maybe not your case.\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self,imgs_path,labels_path, train_flag = True):\n",
    "#         self.imgs_path = imgs_path\n",
    "#         self.labels_path = labels_path\n",
    "#         self.train_flag = train_flag\n",
    "#         file_list = natsorted(glob.glob(self.imgs_path + \"*nii.gz\"), key=lambda y: y.lower())     \n",
    "#         label_list = natsorted(glob.glob(self.labels_path + \"*nii.gz\"), key=lambda y: y.lower())\n",
    "#         self.data = []\n",
    "#         self.label = []\n",
    "#         for img_path in file_list:\n",
    "#             class_name = img_path.split(\"/\")[-1]\n",
    "#             self.data.append([img_path, class_name])\n",
    "#         for label_path in label_list:\n",
    "#                 class_name = label_path.split(\"/\")[-1]\n",
    "#                 self.label.append([label_path, class_name])\n",
    "#         self.train_transforms = Compose(\n",
    "#                 [\n",
    "#                     LoadImaged(keys=[\"image\",\"label\"],reader='nibabelreader'),\n",
    "#                     AddChanneld(keys=[\"image\",\"label\"]),\n",
    "#                     Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "#                     Spacingd(\n",
    "#                         keys=[\"image\",\"label\"],\n",
    "#                         pixdim=spacing,\n",
    "#                         mode=(\"bilinear\"),\n",
    "#                     ),\n",
    "#                     ScaleIntensityd(keys=[\"image\"], minv=-1, maxv=1.0),\n",
    "#                     ScaleIntensityRanged(\n",
    "#                         keys=[\"label\"],\n",
    "#                         a_min=0,\n",
    "#                         a_max=2674,\n",
    "#                         b_min=-1,\n",
    "#                         b_max=1.0,\n",
    "#                         clip=True,\n",
    "#                     ),\n",
    "# #                     ResizeWithPadOrCropd(\n",
    "# #                           keys=[\"image\",\"label\"],\n",
    "# #                           spatial_size=(192,192,168),\n",
    "# #                           constant_values = -1,\n",
    "# #                     ),\n",
    "#                     RandSpatialCropSamplesd(keys=[\"image\",\"label\"],\n",
    "#                                       roi_size = img_size,\n",
    "#                                       num_samples = patch_num,\n",
    "#                                       random_size=False,\n",
    "#                                       ),\n",
    "\n",
    "#                     ToTensord(keys=[\"image\",\"label\"]),\n",
    "#                 ]\n",
    "#             )\n",
    "#         self.test_transforms = Compose(\n",
    "#                 [\n",
    "#                     LoadImaged(keys=[\"image\",\"label\"],reader='nibabelreader'),\n",
    "#                     AddChanneld(keys=[\"image\",\"label\"]),\n",
    "#                     Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "#                     ScaleIntensityd(keys=[\"image\"], minv=-1, maxv=1.0),\n",
    "#                     ScaleIntensityRanged(\n",
    "#                         keys=[\"label\"],\n",
    "#                         a_min=0,\n",
    "#                         a_max=2674,\n",
    "#                         b_min=-1,\n",
    "#                         b_max=1.0,\n",
    "#                         clip=True,\n",
    "#                     ),\n",
    "                    \n",
    "# #                     ResizeWithPadOrCropd(\n",
    "# #                           keys=[\"image\",\"label\"],\n",
    "# #                           spatial_size=(192,192,168),\n",
    "# #                           constant_values = -1,\n",
    "# #                     ),\n",
    "#                     ToTensord(keys=[\"image\",\"label\"]),\n",
    "#                 ]\n",
    "#             ) \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         img_path, class_name = self.data[idx]\n",
    "#         label_path, class_name = self.label[idx]\n",
    "#         cao = {\"image\":img_path,'label':label_path}\n",
    "\n",
    "#         if not self.train_flag:\n",
    "#             affined_data_dict = self.test_transforms(cao)   \n",
    "#             img_tensor = affined_data_dict['image'].to(torch.float)\n",
    "#             label_tensor = affined_data_dict['label'].to(torch.float)\n",
    "#         else:\n",
    "#             affined_data_dict = self.train_transforms(cao)   \n",
    "#             img = np.zeros([patch_num, img_size[0], img_size[1], img_size[2]])\n",
    "#             label = np.zeros([patch_num, img_size[0], img_size[1], img_size[2]])\n",
    "#             for i,after_l in enumerate(affined_data_dict):\n",
    "#                 img[i,:,:,:] = after_l['image']\n",
    "#                 label[i,:,:,:] = after_l['label']\n",
    "#             img_tensor = torch.unsqueeze(torch.from_numpy(img.copy()), 1).to(torch.float)\n",
    "#             label_tensor = torch.unsqueeze(torch.from_numpy(label.copy()), 1).to(torch.float)\n",
    "        \n",
    "#         return img_tensor,label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3f0d8",
   "metadata": {},
   "source": [
    "# Build the MC-IDDPM process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9698a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These three parameters: training steps number, learning variance or not (using improved DDPM or original DDPM), and inference \n",
    "# timesteps number (only effective when using improved DDPM)\n",
    "diffusion_steps=1000\n",
    "learn_sigma=True\n",
    "timestep_respacing=[50]\n",
    "\n",
    "# Don't toch these parameters, they are irrelant to the image synthesis\n",
    "sigma_small=False\n",
    "class_cond=False\n",
    "noise_schedule='linear'\n",
    "use_kl=False\n",
    "predict_xstart=False\n",
    "rescale_timesteps=True\n",
    "rescale_learned_sigmas=True\n",
    "use_checkpoint=False\n",
    "\n",
    "\n",
    "diffusion = create_gaussian_diffusion(\n",
    "    steps=diffusion_steps,\n",
    "    learn_sigma=learn_sigma,\n",
    "    sigma_small=sigma_small,\n",
    "    noise_schedule=noise_schedule,\n",
    "    use_kl=use_kl,\n",
    "    predict_xstart=predict_xstart,\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    rescale_learned_sigmas=rescale_learned_sigmas,\n",
    "    timestep_respacing=timestep_respacing,\n",
    ")\n",
    "schedule_sampler = UniformSampler(diffusion)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e1924",
   "metadata": {},
   "source": [
    "# Build the MC-IDDPM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6250173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Here enter your network parameters:num_channels means the initial channels in each block,\n",
    "# channel_mult means the multipliers of the channels (in this case, 128,128,256,256,512,512 for the first to the sixth block),\n",
    "# attention_resulution means we use the transformer blocks in the third to the sixth block\n",
    "# number of heads, window size in each transformer block\n",
    "# \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_channels=64\n",
    "attention_resolutions=\"32,16,8\"\n",
    "channel_mult = (1, 2, 3, 4)\n",
    "num_heads=[4,4,8,16]\n",
    "window_size = [[4,4,4],[4,4,4],[4,4,2],[4,4,2]]\n",
    "num_res_blocks = [2,2,2,2]\n",
    "sample_kernel=([2,2,2],[2,2,1],[2,2,1],[2,2,1]),\n",
    "\n",
    "attention_ds = []\n",
    "for res in attention_resolutions.split(\",\"):\n",
    "    attention_ds.append(int(res))\n",
    "class_cond = False\n",
    "use_scale_shift_norm=True\n",
    "resblock_updown = False\n",
    "dropout = 0\n",
    "\n",
    "from network.Diffusion_model_transformer import *\n",
    "A_to_B_model = SwinVITModel(\n",
    "          image_size=img_size,\n",
    "          in_channels=2,\n",
    "          model_channels=num_channels,\n",
    "          out_channels=2,\n",
    "          dims=3,\n",
    "          sample_kernel = sample_kernel,\n",
    "          num_res_blocks=num_res_blocks,\n",
    "          attention_resolutions=tuple(attention_ds),\n",
    "          dropout=dropout,\n",
    "          channel_mult=channel_mult,\n",
    "          num_classes=None,\n",
    "          use_checkpoint=False,\n",
    "          use_fp16=False,\n",
    "          num_heads=num_heads,\n",
    "          window_size = window_size,\n",
    "          num_head_channels=64,\n",
    "          num_heads_upsample=-1,\n",
    "          use_scale_shift_norm=use_scale_shift_norm,\n",
    "          resblock_updown=resblock_updown,\n",
    "          use_new_attention_order=False,\n",
    "      ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0393fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In case you want to use CNN\n",
    "# from Diffusion_model_Unet import *\n",
    "# A_to_B_model = UNetModel(\n",
    "#         img_size = img_size,\n",
    "#         image_size=image_size,\n",
    "#         in_channels=2,\n",
    "#         model_channels=num_channels,\n",
    "#         out_channels=2,\n",
    "#         dims = 3,\n",
    "#         num_res_blocks=num_res_blocks[0],\n",
    "#         attention_resolutions=tuple(attention_ds),\n",
    "#         dropout=0.,\n",
    "#         sample_kernel=sample_kernel,\n",
    "#         channel_mult=channel_mult,\n",
    "#         num_classes=(128 if class_cond else None),\n",
    "#         use_checkpoint=False,\n",
    "#         use_fp16=False,\n",
    "#         num_heads=4,\n",
    "#         num_head_channels=64,\n",
    "#         num_heads_upsample=-1,\n",
    "#         use_scale_shift_norm=use_scale_shift_norm,\n",
    "#         resblock_updown=False,\n",
    "#         use_new_attention_order=False,\n",
    "#     ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12c7c5",
   "metadata": {},
   "source": [
    "# Call the optimizer and ready for start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e983d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter number is 54438954\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in A_to_B_model.parameters())\n",
    "print('parameter number is '+str(pytorch_total_params))\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.AdamW(A_to_B_model.parameters(), lr=2e-5,weight_decay = 1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb44eb1",
   "metadata": {},
   "source": [
    "# Build the training function. Run the training function once = one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6283a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explain the training process\n",
    "def train(model, optimizer,data_loader1, loss_history):\n",
    "    \n",
    "    #1: set the model to training mode\n",
    "    model.train()\n",
    "    total_samples = len(data_loader1.dataset)\n",
    "    A_to_B_loss_sum = []\n",
    "    total_time = 0\n",
    "    \n",
    "    #2: Loop the whole dataset, x1 (traindata) is the image batch\n",
    "    for i, (x1,y1) in enumerate(data_loader1):\n",
    "\n",
    "        traintarget = y1.view(-1,1,img_size[0],img_size[1],img_size[2]).to(device)\n",
    "        \n",
    "        traincondition = x1.view(-1,1,img_size[0],img_size[1],img_size[2]).to(device)\n",
    "        \n",
    "        #3: extract random timestep for training\n",
    "        t, weights = schedule_sampler.sample(traincondition.shape[0], device)\n",
    "\n",
    "        aa = time.time()\n",
    "        \n",
    "        #4: Optimize the TDM network\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            all_loss = diffusion.training_losses(A_to_B_model,traintarget,traincondition, t)\n",
    "            A_to_B_loss = (all_loss[\"loss\"] * weights).mean()\n",
    "\n",
    "            A_to_B_loss_sum.append(all_loss[\"loss\"].mean().detach().cpu().numpy())\n",
    "\n",
    "        scaler.scale(A_to_B_loss).backward()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "        \n",
    "        #5:print out the intermediate loss for every 100 batches\n",
    "        total_time += time.time()-aa\n",
    "        if i % 30 == 0:\n",
    "            print('optimization time: '+ str(time.time()-aa))\n",
    "            print('[' +  '{:5}'.format(i * BATCH_SIZE_TRAIN) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader1)) + '%)]  A_to_B_Loss: ' +\n",
    "                  '{:6.7f}'.format(np.nanmean(A_to_B_loss_sum)))\n",
    "\n",
    "    #6: print out the average loss for this epoch\n",
    "    average_loss = np.nanmean(A_to_B_loss_sum) \n",
    "    loss_history.append(average_loss)\n",
    "    print(\"Total time per sample is: \"+str(total_time))\n",
    "    print('Averaged loss is: '+ str(average_loss))\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b32db",
   "metadata": {},
   "source": [
    "# Build the testing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the window sliding method to translate the whole MRI to CT volume. Must used it.\n",
    "# For example, if your whole volume is 64x64x64, and our window size is 64x64x4, so the function will automatically sliding down\n",
    "# the whole volume with a certain overlapping ratio\n",
    "\n",
    "# The window size (img_size) is shown in the \"Build the data loader using the monai library\" section.\n",
    "# img_size: the size of sliding window\n",
    "# img_num: the number of sliding window in each process, only related to your gpu memory, it will still run through the whole volume\n",
    "# overlap: the overlapping ratio\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "img_num = 12\n",
    "overlap = 0.5\n",
    "inferer = SlidingWindowInferer(img_size, img_num, overlap=overlap, mode ='constant')\n",
    "def diffusion_sampling(condition, model):\n",
    "    sampled_images = diffusion.p_sample_loop(model,(condition.shape[0], 1,\n",
    "                                                    condition.shape[2], condition.shape[3],condition.shape[4]),\n",
    "                                                    condition = condition,clip_denoised=True)\n",
    "    return sampled_images\n",
    "\n",
    "# Run the evaluate function will translate the MRI to CT and will be save to a folder in MAT format\n",
    "def evaluate(model,epoch,path,data_loader1,best_loss):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    true = []\n",
    "    img = []\n",
    "    loss_all = []\n",
    "    aa = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (x1,y1) in enumerate(data_loader1):\n",
    "                # target is the target CT\n",
    "                # condition is the input MRI\n",
    "                # sampled_images is the synthetic CT\n",
    "                target = y1.to(device)            \n",
    "                condition = x1.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                      sampled_images = inferer(condition,diffusion_sampling,model)\n",
    "                loss = metric(sampled_images,target)\n",
    "                print('L1 loss: '+ str(loss))\n",
    "                img.append(x1.cpu().numpy())\n",
    "                true.append(target.cpu().numpy())\n",
    "                prediction.append(sampled_images.cpu().numpy())    \n",
    "                loss_all.append(loss.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        print('optimization time: '+ str(1*(time.time()-aa)))  \n",
    "        # The save code, you can replace it by your code for other files, e.g. nii or dicom\n",
    "        data = {\"img\":img,'label':true,'prediction':prediction,'loss':np.mean(loss_all)}\n",
    "        scipy.io.savemat(path+ 'test_example_epoch'+str(epoch)+'.mat',data)\n",
    "        if np.mean(loss_all) < best_loss:\n",
    "            scipy.io.savemat(path+ 'all_final_test_another.mat',data)\n",
    "        return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc474b1",
   "metadata": {},
   "source": [
    "# Start the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311f54bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "optimization time: 7.374521017074585\n",
      "[    0/    3 (  0%)]  A_to_B_Loss: 1.0089188\n",
      "Total time per sample is: 7.374521017074585\n",
      "Averaged loss is: 1.0089188\n",
      "Execution time:  7.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Local\\Temp/ipykernel_25676/2382699765.py\", line 39, in <module>\n",
      "    average_loss = evaluate(A_to_B_model,epoch,path,test_loader1,best_loss)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Local\\Temp/ipykernel_25676/2629023329.py\", line 35, in evaluate\n",
      "    sampled_images = inferer(condition,diffusion_sampling,model)\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\monai\\inferers\\inferer.py\", line 192, in __call__\n",
      "    return sliding_window_inference(  # type: ignore\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\monai\\inferers\\utils.py\", line 176, in sliding_window_inference\n",
      "    seg_prob_out = predictor(window_data, *args, **kwargs)  # batched patch segmentation\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Local\\Temp/ipykernel_25676/2629023329.py\", line 14, in diffusion_sampling\n",
      "    sampled_images = diffusion.p_sample_loop(model,(condition.shape[0], 1,\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\", line 533, in p_sample_loop\n",
      "    for sample in self.p_sample_loop_progressive(\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\", line 586, in p_sample_loop_progressive\n",
      "    out = self.p_sample(\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\", line 481, in p_sample\n",
      "    out = self.p_mean_variance(\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\respace.py\", line 92, in p_mean_variance\n",
      "    return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\", line 322, in p_mean_variance\n",
      "    model_output = model(x_input, self._scale_timesteps(t), **model_kwargs)\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\respace.py\", line 123, in __call__\n",
      "    return self.model(x, new_ts, **kwargs)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\", line 603, in forward\n",
      "    h = module(h, emb)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\", line 46, in forward\n",
      "    x = layer(x)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\", line 77, in forward\n",
      "    x = self.conv(x)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\", line 607, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\pshaoya\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\", line 602, in _conv_forward\n",
      "    return F.conv3d(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\pshaoya\\Anaconda3\\envs\\DL\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25676/2382699765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0maverage_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_to_B_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loader1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maverage_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25676/2629023329.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, epoch, path, data_loader1, best_loss)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                       \u001b[0msampled_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minferer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiffusion_sampling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\monai\\inferers\\inferer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m         return sliding_window_inference(  # type: ignore\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\monai\\inferers\\utils.py\u001b[0m in \u001b[0;36msliding_window_inference\u001b[1;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mwindow_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwin_slice\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwin_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munravel_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mseg_prob_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# batched patch segmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25676/2629023329.py\u001b[0m in \u001b[0;36mdiffusion_sampling\u001b[1;34m(condition, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiffusion_sampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     sampled_images = diffusion.p_sample_loop(model,(condition.shape[0], 1,\n\u001b[0m\u001b[0;32m     15\u001b[0m                                                     condition.shape[2], condition.shape[3],condition.shape[4]),\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\u001b[0m in \u001b[0;36mp_sample_loop\u001b[1;34m(self, model, shape, condition, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         for sample in self.p_sample_loop_progressive(\n\u001b[0m\u001b[0;32m    534\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\u001b[0m in \u001b[0;36mp_sample_loop_progressive\u001b[1;34m(self, model, shape, condition, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 out = self.p_sample(\n\u001b[0m\u001b[0;32m    587\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\u001b[0m in \u001b[0;36mp_sample\u001b[1;34m(self, model, x, t, condition, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \"\"\"\n\u001b[1;32m--> 481\u001b[1;33m         out = self.p_mean_variance(\n\u001b[0m\u001b[0;32m    482\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\respace.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_mean_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\GaussianDiffusion.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, x, t, condition, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scale_timesteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\diffusion\\respace.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mnew_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1000.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_num_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, timesteps, cond, null_cond_prob, y)\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, emb)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Research\\Diffusion model MRI-CT\\github\\network\\Diffusion_model_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    601\u001b[0m             )\n\u001b[1;32m--> 602\u001b[1;33m         return F.conv3d(\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2076\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Enter your data folder\n",
    "training_set1 = CustomDataset('C:\\Research\\Diffusion model MRI-CT\\github\\MRI_to_CT_brain_for_dosimetric/imagesTr/',\n",
    "                              'C:\\Research\\Diffusion model MRI-CT\\github\\MRI_to_CT_brain_for_dosimetric/imagesTr/',train_flag=True)\n",
    "\n",
    "testing_set1 = CustomDataset('C:\\Research\\Diffusion model MRI-CT\\github\\MRI_to_CT_brain_for_dosimetric/imagesTs/',\n",
    "                              'C:\\Research\\Diffusion model MRI-CT\\github\\MRI_to_CT_brain_for_dosimetric/imagesTs/',train_flag=False)\n",
    "# Enter your data reader parameters\n",
    "train_params = {'batch_size': BATCH_SIZE_TRAIN,\n",
    "          'shuffle': True,\n",
    "          'pin_memory': True,\n",
    "          'drop_last': False}\n",
    "train_loader1 = torch.utils.data.DataLoader(training_set1, **train_params)\n",
    "\n",
    "test_params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'pin_memory': True,\n",
    "          'drop_last': False}\n",
    "test_loader1 = torch.utils.data.DataLoader(testing_set1, **test_params)\n",
    "\n",
    "# Enter your total number of epoch\n",
    "N_EPOCHS = 500\n",
    "\n",
    "# Enter the address you save the checkpoint and the evaluation examples\n",
    "path =\"C:/Research/Diffusion model MRI-CT/github/result/Brain_MRI_CT_SwinVIT_IDDPM_final_all_all_slices/\"\n",
    "A_to_B_PATH = path+'A_to_B_ViTRes1.pt' # Use your own path\n",
    "best_loss = 1\n",
    "if not os.path.exists(path):\n",
    "  os.makedirs(path) \n",
    "train_loss_history, test_loss_history = [], []\n",
    "\n",
    "# Uncomment this when you resume the checkpoint\n",
    "#A_to_B_model.load_state_dict(torch.load(A_to_B_PATH),strict=False) \n",
    "for epoch in range(0, N_EPOCHS): \n",
    "    print('Epoch:', epoch)\n",
    "    start_time = time.time() \n",
    "    train(A_to_B_model, optimizer,train_loader1, train_loss_history,)\n",
    "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
    "    if epoch % 5 == 0:\n",
    "        average_loss = evaluate(A_to_B_model,epoch,path,test_loader1,best_loss)\n",
    "        if average_loss < best_loss:\n",
    "            print('Save the latest best model')\n",
    "            torch.save(A_to_B_model.state_dict(), A_to_B_PATH)\n",
    "            best_loss = average_loss\n",
    "print('Execution time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dd4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7dfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
